\documentclass{paper}

%header/footer
\lhead{}
\chead{}
\rhead{}

\begin{document}

\pagenumbering{roman}
\setcounter{page}{-1}

\setlength{\parindent}{0pt}

\title{PUPC}
\author{Benjamin Chen, Michael Diao, Jonathan Huang, Jack Sun and Jason Ye}
\date{\today}
\maketitle

\tableofcontents
\fancyfoot{}

\newpage

\chapter{Laser and Plasma Physics}
\pagenumbering{arabic}

\setcounter{section}{1}
\section{Theory}
\subsection{Plasma Oscillation}

As the electrons on either side of the slab move towards the origin, the slab of ions starts to shrink. So let us denote by $x_e$ the position of an electron at $x = d$ in the initial state.

Using Gauss's law on a cylindrical section of the infinite slab of ions centered at $x=0$ and whose faces lie on the planes of electrons that border the slab, we find that
\begin{align*}
    E\cdot 2A &= \dfrac{Q}{\epsilon_0}
    = \dfrac{2x_eA \cdot n_ee}{\epsilon_0}\\
    \implies 
    F &= -e \cdot \dfrac{x_e \cdot n_ee}{\epsilon_0}\\
    \implies 
    \dfrac{d^2x_e}{dt^2} &= -\dfrac{n_ee^2}{m_e\epsilon_0}x_e.
\end{align*}
This describes simple harmonic motion with period $\omega_{pl} = \sqrt{\dfrac{n_ee^2}{m_e\epsilon_0}}$.

\subsection{The Langmuir Wave: a Warm Model for the Plasma Oscillation}

    The differential equation can be obtained by calculating the partial derivative of (3) with respect to $t$ and the partial derivative of (4) with respect to $x$:

    \[
    \small
    \def\arraystretch{1.5}
    \begin{array}{r c l c r c l}
        \dfrac{\partial n^\prime}{\partial t} + n_0\dfrac{\partial v^\prime}{\partial x} &= &0
        &\implies&\dfrac{\partial^2 n^\prime}{\partial t^2} &=&-n_0\dfrac{\partial^2 v^\prime}{\partial x\partial t} \\ %from (3)
        % \dfrac{\partial n^\prime}{\partial x}\cdot \dfrac{m \partial v^\prime}{\partial t} + m \cdot \dfrac{n \partial^2 v}{\partial x\partial t} &= &\dfrac{\partial^2 n}{\partial x^2} E_{kin} + \dfrac{\partial f}{\partial x} 
        %&\implies& placeholder &= &placeholder \\%from (4)
        \dfrac{\partial n^\prime}{\partial x}\cdot \dfrac{E_{kin}}{m} - \dfrac{f}{m}  &= &-n_0\dfrac{\partial v^\prime}{\partial t} 
        &\implies& \dfrac{\partial^2 n^\prime}{\partial x^2} \cdot \dfrac{E_{kin}}{m} - \dfrac{\partial f}{\partial x} \cdot \dfrac{1}{m}&= & -n_0\dfrac{\partial^2 v^\prime}{\partial x\partial t}\\
        & & &\implies &\dfrac{\partial^2 n^\prime}{\partial x^2} E_{kin} - m\dfrac{\partial^2 n^\prime}{\partial t^2} &= &\dfrac{\partial f}{\partial x}\\
    \end{array}
    \]
    
    To avoid confusion, we will use $q_e$ to represent the charge on an electron.

    Given that \(n_e^\prime = Ae^{i(w t - k x)}\), and expressing the charge density in terms of $n_e^\prime$:
    \begin{align*}
        \frac{\partial^2 n_e^\prime}{\partial x^2} &= (ik)^2 Ae^{i(w t - k x)}\\
        \frac{\partial^2 n_e^\prime}{\partial t^2} &= (iw)^2 Ae^{i(w t - k x)}\\
        \rho_e &= n_e^\prime q_e\\
        &= Ae^{i(w t - k x)} q_e
    \end{align*}
    
    Using the equations derived above, we have
    \[
    \small
    \def\arraystretch{1.5}
    \begin{array}{r c l c r c l}
        \dfrac{\partial f}{\partial x} &=& e\dfrac{\partial E}{\partial x} &\implies&
        \dfrac{\partial^2 n^\prime}{\partial x^2} E_{kin} - m\dfrac{\partial^2 n^\prime}{\partial t^2} &=& \dfrac{e\rho_e}{\epsilon_0}\\
        & & &\implies& (ik)^2 Ae^{i(w t - k x)} \cdot E_{kin} - (iw)^2 Ae^{i(w t - k x)} \cdot m &=& \dfrac{q_e \cdot Ae^{i(w t - k x)} q_e}{\epsilon_0}\\
        & & &\implies& -k^2 E_{kin} + w^2 m &=& \dfrac{q_e^2}{\epsilon_0}\\
        & & &\implies& w &=& \sqrt{\dfrac{k^2E_{kin} + \dfrac{q_e^2}{\epsilon_0}}{m}}\\
        & & &\implies& f &=& \dfrac{1}{2\pi} \sqrt{\dfrac{k^2E_{kin} + \dfrac{q_e^2}{\epsilon_0}}{m}}
        %@Ben @Jonathan what the hell is the w_{pl} mentioned in the problem?
    \end{array}
    \]
    
    Rewriting, we have
    \[w = \sqrt{\dfrac{k^2v_{thm} + \dfrac{q_e^2}{\epsilon_0}}{m}}\]
    

\chapter{Entropy and Statistical Mechanics}
\section{Entropy as information}
\subsection{Quantifying the amount of information in the answer to a question}

\par Despite the minus sign, the given definition of entropy is always non-negative; as probabilities are always no more than 1, their logs are non-positive, so the minus sign flips it back to non-negative.

\begin{claim}
    The minimum and maximum values of $I$ are $0$ and $\log N$, respectively.
\end{claim}
    
\begin{proof}
    First, we show $0$ and $\log N$ are attainable values of $I$. In the first case, consider when $p_1=1$ and all other values of $p_i$ for $2\leq i \leq N$ equal $0$. Then
        \begin{align*}
            I
            &= -\sum\limits_{i=1}^{N} p_i\log(p_i)\\
            &= -\lp p_1\log(p_1) + \sum\limits_{i=2}^{N} p_i\log(p_i) \rp 
            = 0.
        \end{align*}
    Since we previously established that $I$ is always non-negative, $I=0$ must be the minimum.
    
    To obtain $I=\log N$, let $p_i=\frac{1}{N}$ for all $1\leq i\leq N$. Then 
        \[
            I
            = -\sum\limits_{i=1}^{N} \frac{1}{N}\log\lp\frac{1}{N}\rp
            = -\log\lp\frac{1}{N}\rp = \log N.
        \]
    Now we show that $I=\log N$ is indeed maximal. From the definition of $I$,
        \[
            I
            = -\sum\limits_{i=1}^{N} p_i\log\lp p_i \rp
            = -\log\lp \prod\limits_{i=1}^{N} p_i^{p_i} \rp.
        \]
    In order to maximize $I$, we must minimize $\prod\limits_{i=1}^{N} p_i^{p_i}$. Note that, by the inequality of Geometric and Harmonic means, we have
        \[
            \prod\limits_{i=1}^{N}p_i^{p_i} 
            = \lp \prod\limits_{i=1}^{N}p_i^{p_i} \rp^{\sum\limits_{i=1}^{N} p_i}
            \geq \frac{\lp\sum\limits_{i=1}^{N} p_i\rp}{\lp\sum\limits_{i=1}^{N} \frac{p_i}{p_i}\rp}
            = \frac{1}{N},
        \]
    with equality at $p_1=p_2=\ldots=p_N$. Thus, we find that
        \[
            I
            = -\log\lp \prod\limits_{i=1}^{N} p_i^{p_i}\rp \leq -\log\lp\frac{1}{N}\rp,
        \]
    and thus the maximum value of $I$ is $\log N$ as desired.
\end{proof}

Using a different base $b$ for the logarithms scales it by a factor of $\log(b)$; all of the terms have a logarithmic term of base $e$, and change of base gives us that 
    \[
        \frac{\log_b(p)}{\log(p)}
        = \log(b).
    \]

This definition of entropy obviously satisfies the second and third criteria; the equation only involves the probabilities of the outcomes, and, as shown above, the entropy is largest when all probabilities are equal.  Furthermore, the first criterion is satisfied by the fact that !!!!!NOT COMPLETED!!!!!!%what the hell

\section{Where does entropy show up in Statistical Mechanics?}
\subsection{An example: rigid chain and the "force" that entropy causes}

\begin{enumerate}
    \item The free end of the chain can be at any integer position $x$ such that $-N/2 \leq x \leq N/2$.

    \item A single link of the chain can either be oriented towards the negative or positive x-direction.  Thus, there are $2^N$ possible configurations for the entire chain.
    
    \item For the free end of the chain to lie at $x=n$, there must be $\frac{N}{2}+n$ chain segments oriented in the positive direction, and $\frac{N}{2}-n$ segments in the negative direction.  The number of ways this can happen is
        \[
            \binom{N}{\frac{N}{2}-n}
            = \frac{N!}{\lp\frac{N}{2}-n\rp! \cdot \lp\frac{N}{2}+n\rp!}
        \]
    
    \item The probability is just the number of outcomes where $x=n$ over the total number of outcomes, so
        \[
            p(x=n)
            = \frac{N!}{2^N \cdot \lp\frac{N}{2}-n\rp! \cdot \lp\frac{N}{2}+n\rp!}
        \]
\end{enumerate}

\par As there are $\binom{N}{\frac{N}{2}-n}$ different equally likely positions for the state of the chain, we have
    \begin{align*}
        \sigma_{sys}(x=n) 
        &= \log\lp \frac{N!}{\lp\frac{N}{2}-n\rp! \cdot \lp\frac{N}{2}+n\rp!}\rp \\
        &= \log\lp 2^N p(x=n) \rp
    \end{align*}

\subsubsection{Stirling's approximation: a way to simplify the formula}

\par Plugging in Stirling's approximation to the expression in \#4 gives:
    \begin{align*}
        p(x=n) 
        &= \frac{1}{2^N} \cdot \frac{\sqrt{2\pi N}}{\sqrt{2\pi \lp\frac{N}{2} + n\rp} \cdot \sqrt{2\pi \lp\frac{N}{2} - n\rp}} \cdot \frac{\lp\frac{N}{e}\rp^N}{\lp\frac{\frac{N}{2} + n}{e}\rp^{\lp\frac{N}{2} + n\rp} \cdot \lp\frac{\frac{N}{2} - n}{e}\rp^{\lp\frac{N}{2} - n\rp}}\\
        &= \sqrt{\frac{N}{2\pi \lp\frac{N^2}{4} - n^2\rp}} \cdot \lp\frac{N}{2}\rp^N \cdot \lp\frac{N^2}{4} - n^2\rp^{-\frac{N}{2}} \cdot \lp\frac{N-2n}{N+2n}\rp^n \\
        &= \sqrt{\frac{N}{2\pi \lp\frac{N^2}{4} - n^2\rp}} \cdot \lp1 - \frac{4n^2}{N^2}\rp^{-\frac{N}{2}} \cdot \lp1 - \frac{4n}{N+2n}\rp^n \\
        &= \sqrt{\frac{2}{\pi N}} \cdot \lp1 - \frac{4n^2}{N^2}\rp^{-\frac{N+1}{2}} \cdot \lp1 - \frac{4n}{N+2n}\rp^n \\
        ...\\
        &= \frac{k}{\sqrt{N}} \exp \lp -\frac{4n^2}{N} \rp
    \end{align*}
    %Needs to be finished

\par Therefor, we have
    \begin{align*}
        \sigma_{Sys}(x=n) &= \log(2^N p(x=n)) \\
        &= \log \lp \frac{k \cdot 2^N}{\sqrt{N}} \cdot \exp \lp -\frac{4n^2}{N} \rp \rp \\
        &= \log \lp \frac{k \cdot 2^N}{\sqrt{N}} \rp - \frac{4n^2}{N}
    \end{align*}

\subsection{An abstract derivation of entropy in statistical mechanics}
\subsubsection{A derivation}

\par The probability that $E_{Sys} = E_k$ is equal to the probability that the system is in any state $\omega^S_k$ with energy $E_k$.  As there are $\exp(\sigma_{Sys}(E_k))$ states of the system with energy $E_k$, and the probability of each one is $\frac{1}{\zeta_2}\exp\lp-\frac{D_k}{\tau}\rp$, we have
    \begin{align*}
        p(E_{Sys} = E_k) 
        &= \exp(\sigma_{Sys}(E_k) \cdot \frac{1}{\zeta_2} \exp \lp - \frac{E_k}{\tau} \rp \\
        &= \frac{1}{\zeta_2} \exp \lp - \lp \frac{E_k}{\tau} - \sigma_{Sys}(E_k) \rp \rp \\
        &= \frac{1}{\zeta_2} \exp \lp -\beta \lp E_k - \tau \sigma_{Sys}(E_k) \rp \rp.
    \end{align*}
    
\par There are two "competing" effects here -- the number of states for a given energy, and the energy itself.  On one hand, a lower energy is favored, as lowering $E_k$ will decrease the $E_k$ term in the definition of $F(E)$.  However, energies with more states will also be favored, as increasing $\sigma_{Sys}(E_k)$ will lower the $-\tau \sigma_{Sys}(E_k)$ term in $F(E)$.  Thus, the most probable state, when $F(E)$ is minimized, is at a balance between when $E_k$ is minimized and when $\sigma_{Sys}(E_k)$ is maximized.
    
\subsubsection{A pause for reflection}

\par The central assumptions in the derivation are that 1. energy is conserved between the system and reservoir, 2. all of the possible states of the system and reservoir are equally likely, and 3. $E_k \ll E$ to make the approximation work.

\subsection{Why is our definition of temperature reasonable?}
\subsubsection{A physical example: magnetic spins}

\par This physical example is analogous to the chain in the previous section.  Each magnetic spin can be compared to a segment in the chain.  The two states of spin up and spin down correspond to the segment being either in the positive or negative direction.  The total energy of the magnetic spins correspond to the position of the chain's end, with and energy of $E$ corresponding to a position of $\frac{E}{2\eta}$.  

\par Therefore, the shape of $\sigma_{Sys}(E)$ is the same as that of $\sigma_{Sys}(x)$ from the chain problem, except scaled by a factor of $2\eta$, giving zeros at $E = \pm N\eta$, and a maximum at $E=0$.

\subsubsection{A large reservoir or spins coupled to a smaller system}

\par 

\end{document}
















%jason pls